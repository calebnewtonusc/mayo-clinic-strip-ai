{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability Analysis\n",
    "\n",
    "This notebook demonstrates various interpretability techniques for understanding model predictions:\n",
    "- Grad-CAM and Grad-CAM++\n",
    "- Feature space visualization (t-SNE, UMAP, PCA)\n",
    "- Guided backpropagation\n",
    "- Feature separability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.dataset import StrokeDataset\n",
    "from src.data.augmentation import get_val_augmentation\n",
    "from src.models.cnn import ResNetClassifier\n",
    "from src.visualization.gradcam import GradCAM, GradCAMPlusPlus, get_target_layer, overlay_heatmap_on_image\n",
    "from src.visualization.features import extract_features, compute_tsne, compute_pca, plot_embedding\n",
    "from src.utils.helpers import load_config, get_device\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "checkpoint_path = '../experiments/checkpoints/best_model.pth'\n",
    "config_path = '../config/default_config.yaml'\n",
    "data_dir = '../data/processed'\n",
    "\n",
    "# Load config\n",
    "config = load_config(config_path)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = ResNetClassifier(\n",
    "    arch=config['model']['architecture'],\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = StrokeDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='val',\n",
    "    split_file='../data/splits/val.json',\n",
    "    transform=get_val_augmentation(config['data']['image_size'])\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grad-CAM Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Grad-CAM\n",
    "target_layer = get_target_layer(model, 'resnet')\n",
    "gradcam = GradCAM(model, target_layer)\n",
    "\n",
    "# Select a random sample\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "image, true_label = dataset[idx]\n",
    "\n",
    "# Generate Grad-CAM\n",
    "image_input = image.unsqueeze(0).to(device)\n",
    "cam = gradcam.generate_cam(image_input)\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image_input)\n",
    "    pred_label = output.argmax(dim=1).item()\n",
    "    confidence = torch.softmax(output, dim=1)[0, pred_label].item()\n",
    "\n",
    "# Prepare image for visualization\n",
    "img_display = image.permute(1, 2, 0).cpu().numpy()\n",
    "if img_display.max() <= 1.0:\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_display = std * img_display + mean\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "img_display = (img_display * 255).astype(np.uint8)\n",
    "\n",
    "# Overlay\n",
    "overlayed = overlay_heatmap_on_image(img_display, cam, alpha=0.4)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "class_names = ['CE', 'LAA']\n",
    "\n",
    "axes[0].imshow(img_display)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cam, cmap='jet')\n",
    "axes[1].set_title('Grad-CAM Heatmap')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(overlayed)\n",
    "axes[2].set_title('Overlayed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "color = 'green' if pred_label == true_label else 'red'\n",
    "title = f\"True: {class_names[true_label]} | Pred: {class_names[pred_label]} ({confidence:.2%})\"\n",
    "fig.suptitle(title, fontsize=14, fontweight='bold', color=color)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grad-CAM++ Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Grad-CAM and Grad-CAM++\n",
    "gradcam_pp = GradCAMPlusPlus(model, target_layer)\n",
    "\n",
    "# Generate both\n",
    "cam_regular = gradcam.generate_cam(image_input)\n",
    "cam_plus = gradcam_pp.generate_cam(image_input)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img_display)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(overlay_heatmap_on_image(img_display, cam_regular, alpha=0.4))\n",
    "axes[1].set_title('Grad-CAM')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(overlay_heatmap_on_image(img_display, cam_plus, alpha=0.4))\n",
    "axes[2].set_title('Grad-CAM++')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM vs Grad-CAM++', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "features, labels = extract_features(model, dataloader, device)\n",
    "\n",
    "print(f\"Feature shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute t-SNE\n",
    "tsne_embedded = compute_tsne(features)\n",
    "plot_embedding(tsne_embedded, labels, title=\"t-SNE Feature Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "pca_embedded = compute_pca(features)\n",
    "plot_embedding(pca_embedded, labels, title=\"PCA Feature Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Separability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.features import analyze_feature_separability\n",
    "\n",
    "separability = analyze_feature_separability(features, labels)\n",
    "\n",
    "print(\"Feature Separability Metrics:\")\n",
    "for metric, value in separability.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multiple Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Grad-CAM for multiple samples\n",
    "n_samples = 6\n",
    "indices = np.random.choice(len(dataset), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 3, figsize=(12, 4*n_samples))\n",
    "\n",
    "for row, idx in enumerate(indices):\n",
    "    image, true_label = dataset[idx]\n",
    "    image_input = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate Grad-CAM\n",
    "    cam = gradcam.generate_cam(image_input)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_input)\n",
    "        pred_label = output.argmax(dim=1).item()\n",
    "        confidence = torch.softmax(output, dim=1)[0, pred_label].item()\n",
    "    \n",
    "    # Prepare image\n",
    "    img_display = image.permute(1, 2, 0).cpu().numpy()\n",
    "    if img_display.max() <= 1.0:\n",
    "        img_display = std * img_display + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "    img_display = (img_display * 255).astype(np.uint8)\n",
    "    \n",
    "    # Overlay\n",
    "    overlayed = overlay_heatmap_on_image(img_display, cam, alpha=0.4)\n",
    "    \n",
    "    # Plot\n",
    "    axes[row, 0].imshow(img_display)\n",
    "    axes[row, 0].axis('off')\n",
    "    axes[row, 1].imshow(cam, cmap='jet')\n",
    "    axes[row, 1].axis('off')\n",
    "    axes[row, 2].imshow(overlayed)\n",
    "    axes[row, 2].axis('off')\n",
    "    \n",
    "    # Title for row\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    axes[row, 0].set_ylabel(\n",
    "        f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]} ({confidence:.2%})\",\n",
    "        fontsize=10,\n",
    "        color=color,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "axes[0, 0].set_title('Original', fontsize=12)\n",
    "axes[0, 1].set_title('Grad-CAM', fontsize=12)\n",
    "axes[0, 2].set_title('Overlay', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clinical Interpretation\n",
    "\n",
    "**Questions to consider:**\n",
    "- Does the model focus on clinically relevant regions?\n",
    "- Are the attention patterns consistent with medical knowledge?\n",
    "- Do misclassifications show attention on confounding features?\n",
    "- Is there a pattern in which samples have low confidence?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
