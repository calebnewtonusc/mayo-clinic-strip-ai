{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Quantification\n",
    "\n",
    "This notebook demonstrates uncertainty quantification techniques:\n",
    "- Monte Carlo Dropout\n",
    "- Test-Time Augmentation\n",
    "- Confidence calibration\n",
    "- Identifying uncertain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.dataset import StrokeDataset\n",
    "from src.data.augmentation import get_train_augmentation, get_val_augmentation\n",
    "from src.models.cnn import ResNetClassifier\n",
    "from src.evaluation.uncertainty import (\n",
    "    monte_carlo_dropout, test_time_augmentation,\n",
    "    calculate_confidence_metrics, calibration_curve,\n",
    "    expected_calibration_error, identify_uncertain_samples\n",
    ")\n",
    "from src.utils.helpers import load_config, get_device\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../experiments/checkpoints/best_model.pth'\n",
    "config_path = '../config/default_config.yaml'\n",
    "data_dir = '../data/processed'\n",
    "\n",
    "config = load_config(config_path)\n",
    "device = get_device()\n",
    "\n",
    "# Load model\n",
    "model = ResNetClassifier(\n",
    "    arch=config['model']['architecture'],\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = StrokeDataset(\n",
    "    data_dir=data_dir,\n",
    "    split='val',\n",
    "    split_file='../data/splits/val.json',\n",
    "    transform=get_val_augmentation(config['data']['image_size'])\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MC dropout on first batch\n",
    "images, labels = next(iter(dataloader))\n",
    "\n",
    "n_iterations = 30\n",
    "mean_probs, std_probs, all_predictions = monte_carlo_dropout(\n",
    "    model, images, n_iterations=n_iterations, device=device\n",
    ")\n",
    "\n",
    "print(f\"Mean probabilities shape: {mean_probs.shape}\")\n",
    "print(f\"Std probabilities shape: {std_probs.shape}\")\n",
    "print(f\"All predictions shape: {all_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty for a sample\n",
    "sample_idx = 0\n",
    "sample_predictions = all_predictions[:, sample_idx, :]  # (n_iterations, n_classes)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot prediction distribution\n",
    "axes[0].hist(sample_predictions[:, 0], bins=20, alpha=0.7, label='CE')\n",
    "axes[0].hist(sample_predictions[:, 1], bins=20, alpha=0.7, label='LAA')\n",
    "axes[0].set_xlabel('Probability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('MC Dropout Prediction Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot confidence over iterations\n",
    "max_probs = sample_predictions.max(axis=1)\n",
    "axes[1].plot(max_probs)\n",
    "axes[1].axhline(mean_probs[sample_idx].max(), color='r', linestyle='--', label='Mean')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Max Probability')\n",
    "axes[1].set_title('Confidence Across MC Dropout Iterations')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean confidence: {mean_probs[sample_idx].max():.4f}\")\n",
    "print(f\"Std: {std_probs[sample_idx].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence metrics for entire validation set\n",
    "all_mean_probs = []\n",
    "all_std_probs = []\n",
    "all_predictions_mc = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    mean_probs, std_probs, predictions_mc = monte_carlo_dropout(\n",
    "        model, images, n_iterations=30, device=device\n",
    "    )\n",
    "    \n",
    "    all_mean_probs.append(mean_probs)\n",
    "    all_std_probs.append(std_probs)\n",
    "    all_predictions_mc.append(predictions_mc)\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate\n",
    "mean_probs = np.concatenate(all_mean_probs, axis=0)\n",
    "std_probs = np.concatenate(all_std_probs, axis=0)\n",
    "predictions_mc = np.concatenate(all_predictions_mc, axis=1)\n",
    "labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_confidence_metrics(mean_probs, predictions_mc)\n",
    "\n",
    "print(\"Confidence Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate calibration\n",
    "predictions = mean_probs.argmax(axis=1)\n",
    "confidences = mean_probs.max(axis=1)\n",
    "y_true_binary = (predictions == labels).astype(int)\n",
    "\n",
    "bin_confs, bin_accs = calibration_curve(y_true_binary, confidences, n_bins=10)\n",
    "ece = expected_calibration_error(y_true_binary, confidences)\n",
    "\n",
    "# Plot calibration curve\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax.plot(bin_confs, bin_accs, 'o-', label='Model', markersize=10)\n",
    "ax.set_xlabel('Confidence', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Calibration Curve (Reliability Diagram)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ECE text\n",
    "ax.text(0.05, 0.95, f'ECE: {ece:.4f}', transform=ax.transAxes,\n",
    "       fontsize=12, verticalalignment='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected Calibration Error: {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Identify Uncertain Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify uncertain samples\n",
    "uncertain = identify_uncertain_samples(\n",
    "    mean_probs, predictions_mc,\n",
    "    threshold_confidence=0.7,\n",
    "    threshold_entropy=0.5\n",
    ")\n",
    "\n",
    "print(f\"Uncertain samples: {uncertain.sum()} / {len(uncertain)} ({100*uncertain.sum()/len(uncertain):.1f}%)\")\n",
    "\n",
    "# Compare accuracy on certain vs uncertain\n",
    "certain_mask = ~uncertain\n",
    "\n",
    "certain_acc = (predictions[certain_mask] == labels[certain_mask]).mean()\n",
    "uncertain_acc = (predictions[uncertain] == labels[uncertain]).mean()\n",
    "\n",
    "print(f\"\\nAccuracy on certain samples: {certain_acc:.4f}\")\n",
    "print(f\"Accuracy on uncertain samples: {uncertain_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence vs Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confidence distribution by correctness\n",
    "correct_mask = predictions == labels\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "axes[0].hist(confidences[correct_mask], bins=30, alpha=0.7, label='Correct', density=True)\n",
    "axes[0].hist(confidences[~correct_mask], bins=30, alpha=0.7, label='Incorrect', density=True)\n",
    "axes[0].set_xlabel('Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(confidences[correct_mask], np.ones(correct_mask.sum()),\n",
    "               alpha=0.5, label='Correct', s=20)\n",
    "axes[1].scatter(confidences[~correct_mask], np.zeros((~correct_mask).sum()),\n",
    "               alpha=0.5, label='Incorrect', s=20)\n",
    "axes[1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Correctness', fontsize=12)\n",
    "axes[1].set_title('Confidence vs Correctness', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test-Time Augmentation (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA on a few samples (computationally expensive)\n",
    "augmentation_fn = get_train_augmentation(image_size=224, p=1.0)\n",
    "\n",
    "sample_image, sample_label = dataset[0]\n",
    "sample_image_input = sample_image.unsqueeze(0)\n",
    "\n",
    "tta_mean, tta_std = test_time_augmentation(\n",
    "    model, sample_image_input, augmentation_fn,\n",
    "    n_augmentations=20, device=device\n",
    ")\n",
    "\n",
    "print(\"Test-Time Augmentation Results:\")\n",
    "print(f\"Mean prediction: {tta_mean[0]}\")\n",
    "print(f\"Std: {tta_std[0]}\")\n",
    "print(f\"Predicted class: {tta_mean[0].argmax()}\")\n",
    "print(f\"True class: {sample_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clinical Recommendations\n",
    "\n",
    "**Based on uncertainty analysis:**\n",
    "\n",
    "1. **High Confidence Predictions** (>0.9): Can be used with minimal human oversight\n",
    "2. **Medium Confidence** (0.7-0.9): Recommend expert review\n",
    "3. **Low Confidence** (<0.7): Requires expert diagnosis\n",
    "4. **Uncertain Samples**: Flag for manual review or additional testing\n",
    "\n",
    "**Key Insights:**\n",
    "- Model calibration (ECE) indicates how well confidence matches accuracy\n",
    "- MC dropout reveals epistemic uncertainty (model uncertainty)\n",
    "- TTA reveals aleatoric uncertainty (data uncertainty)\n",
    "- Uncertain predictions often indicate ambiguous cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
